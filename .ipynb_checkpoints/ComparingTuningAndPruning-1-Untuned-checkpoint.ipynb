{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ba95fbd",
   "metadata": {},
   "source": [
    "## Studying the effect of pruning on untuned and tuned models - File for untuned models\n",
    "\n",
    "### Imports and getting data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bc4c1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from keras.layers import Dense, Conv1D, Flatten, GlobalAveragePooling1D, TimeDistributed, LSTM, AveragePooling1D, SimpleRNN, LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c48d5838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.random as rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "021b8d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Usage</th>\n",
       "      <th>LaggingCurrentReactivePower</th>\n",
       "      <th>LeadingCurrentReactivePower</th>\n",
       "      <th>CO2</th>\n",
       "      <th>LaggingCurrentPowerFactor</th>\n",
       "      <th>LeadingCurrentPowerFactor</th>\n",
       "      <th>NSM</th>\n",
       "      <th>WeekStatus</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>...</th>\n",
       "      <th>SeaLevelPressure</th>\n",
       "      <th>CloudCover</th>\n",
       "      <th>Visibility</th>\n",
       "      <th>SolarRadiation</th>\n",
       "      <th>SolarEnergy</th>\n",
       "      <th>UvIndex</th>\n",
       "      <th>Conditions</th>\n",
       "      <th>SunriseHour</th>\n",
       "      <th>LoadType</th>\n",
       "      <th>SunsetHour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.17</td>\n",
       "      <td>2.95</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73.21</td>\n",
       "      <td>100.00</td>\n",
       "      <td>900</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1026.9</td>\n",
       "      <td>3.3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>139.9</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6</td>\n",
       "      <td>Clear</td>\n",
       "      <td>7</td>\n",
       "      <td>Light_Load</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.28</td>\n",
       "      <td>3.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>67.76</td>\n",
       "      <td>100.00</td>\n",
       "      <td>5400</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1026.9</td>\n",
       "      <td>3.3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>139.9</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6</td>\n",
       "      <td>Clear</td>\n",
       "      <td>7</td>\n",
       "      <td>Light_Load</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.46</td>\n",
       "      <td>4.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.14</td>\n",
       "      <td>100.00</td>\n",
       "      <td>9900</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1026.9</td>\n",
       "      <td>3.3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>139.9</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6</td>\n",
       "      <td>Clear</td>\n",
       "      <td>7</td>\n",
       "      <td>Light_Load</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.89</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.40</td>\n",
       "      <td>100.00</td>\n",
       "      <td>14400</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1026.9</td>\n",
       "      <td>3.3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>139.9</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6</td>\n",
       "      <td>Clear</td>\n",
       "      <td>7</td>\n",
       "      <td>Light_Load</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.56</td>\n",
       "      <td>4.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.84</td>\n",
       "      <td>100.00</td>\n",
       "      <td>18900</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1026.9</td>\n",
       "      <td>3.3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>139.9</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6</td>\n",
       "      <td>Clear</td>\n",
       "      <td>7</td>\n",
       "      <td>Light_Load</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7003</th>\n",
       "      <td>3.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.79</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>32.98</td>\n",
       "      <td>64800</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>1035.8</td>\n",
       "      <td>19.6</td>\n",
       "      <td>10.5</td>\n",
       "      <td>143.6</td>\n",
       "      <td>12.5</td>\n",
       "      <td>6</td>\n",
       "      <td>Clear</td>\n",
       "      <td>7</td>\n",
       "      <td>Light_Load</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7004</th>\n",
       "      <td>3.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>21.16</td>\n",
       "      <td>69300</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>1035.8</td>\n",
       "      <td>19.6</td>\n",
       "      <td>10.5</td>\n",
       "      <td>143.6</td>\n",
       "      <td>12.5</td>\n",
       "      <td>6</td>\n",
       "      <td>Clear</td>\n",
       "      <td>7</td>\n",
       "      <td>Light_Load</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7005</th>\n",
       "      <td>3.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.43</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>24.41</td>\n",
       "      <td>73800</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>1035.8</td>\n",
       "      <td>19.6</td>\n",
       "      <td>10.5</td>\n",
       "      <td>143.6</td>\n",
       "      <td>12.5</td>\n",
       "      <td>6</td>\n",
       "      <td>Clear</td>\n",
       "      <td>7</td>\n",
       "      <td>Light_Load</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7006</th>\n",
       "      <td>3.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>24.80</td>\n",
       "      <td>78300</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>1035.8</td>\n",
       "      <td>19.6</td>\n",
       "      <td>10.5</td>\n",
       "      <td>143.6</td>\n",
       "      <td>12.5</td>\n",
       "      <td>6</td>\n",
       "      <td>Clear</td>\n",
       "      <td>7</td>\n",
       "      <td>Light_Load</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7007</th>\n",
       "      <td>3.85</td>\n",
       "      <td>4.86</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62.10</td>\n",
       "      <td>100.00</td>\n",
       "      <td>82800</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>1035.8</td>\n",
       "      <td>19.6</td>\n",
       "      <td>10.5</td>\n",
       "      <td>143.6</td>\n",
       "      <td>12.5</td>\n",
       "      <td>6</td>\n",
       "      <td>Clear</td>\n",
       "      <td>7</td>\n",
       "      <td>Light_Load</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7008 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Usage  LaggingCurrentReactivePower  LeadingCurrentReactivePower  CO2  \\\n",
       "0      3.17                         2.95                         0.00  0.0   \n",
       "1      3.28                         3.56                         0.00  0.0   \n",
       "2      3.46                         4.03                         0.00  0.0   \n",
       "3      3.89                         5.00                         0.00  0.0   \n",
       "4      3.56                         4.07                         0.00  0.0   \n",
       "...     ...                          ...                          ...  ...   \n",
       "7003   3.42                         0.00                         9.79  0.0   \n",
       "7004   3.96                         0.00                        18.29  0.0   \n",
       "7005   3.38                         0.00                        13.43  0.0   \n",
       "7006   3.42                         0.00                        13.36  0.0   \n",
       "7007   3.85                         4.86                         0.00  0.0   \n",
       "\n",
       "      LaggingCurrentPowerFactor  LeadingCurrentPowerFactor    NSM WeekStatus  \\\n",
       "0                         73.21                     100.00    900    Weekday   \n",
       "1                         67.76                     100.00   5400    Weekday   \n",
       "2                         65.14                     100.00   9900    Weekday   \n",
       "3                         61.40                     100.00  14400    Weekday   \n",
       "4                         65.84                     100.00  18900    Weekday   \n",
       "...                         ...                        ...    ...        ...   \n",
       "7003                     100.00                      32.98  64800    Weekday   \n",
       "7004                     100.00                      21.16  69300    Weekday   \n",
       "7005                     100.00                      24.41  73800    Weekday   \n",
       "7006                     100.00                      24.80  78300    Weekday   \n",
       "7007                      62.10                     100.00  82800    Weekday   \n",
       "\n",
       "      Year  Month  ...  SeaLevelPressure  CloudCover  Visibility  \\\n",
       "0     2018      1  ...            1026.9         3.3         9.0   \n",
       "1     2018      1  ...            1026.9         3.3         9.0   \n",
       "2     2018      1  ...            1026.9         3.3         9.0   \n",
       "3     2018      1  ...            1026.9         3.3         9.0   \n",
       "4     2018      1  ...            1026.9         3.3         9.0   \n",
       "...    ...    ...  ...               ...         ...         ...   \n",
       "7003  2018     12  ...            1035.8        19.6        10.5   \n",
       "7004  2018     12  ...            1035.8        19.6        10.5   \n",
       "7005  2018     12  ...            1035.8        19.6        10.5   \n",
       "7006  2018     12  ...            1035.8        19.6        10.5   \n",
       "7007  2018     12  ...            1035.8        19.6        10.5   \n",
       "\n",
       "      SolarRadiation SolarEnergy  UvIndex  Conditions  SunriseHour  \\\n",
       "0              139.9        12.0        6       Clear            7   \n",
       "1              139.9        12.0        6       Clear            7   \n",
       "2              139.9        12.0        6       Clear            7   \n",
       "3              139.9        12.0        6       Clear            7   \n",
       "4              139.9        12.0        6       Clear            7   \n",
       "...              ...         ...      ...         ...          ...   \n",
       "7003           143.6        12.5        6       Clear            7   \n",
       "7004           143.6        12.5        6       Clear            7   \n",
       "7005           143.6        12.5        6       Clear            7   \n",
       "7006           143.6        12.5        6       Clear            7   \n",
       "7007           143.6        12.5        6       Clear            7   \n",
       "\n",
       "        LoadType  SunsetHour  \n",
       "0     Light_Load          17  \n",
       "1     Light_Load          17  \n",
       "2     Light_Load          17  \n",
       "3     Light_Load          17  \n",
       "4     Light_Load          17  \n",
       "...          ...         ...  \n",
       "7003  Light_Load          17  \n",
       "7004  Light_Load          17  \n",
       "7005  Light_Load          17  \n",
       "7006  Light_Load          17  \n",
       "7007  Light_Load          17  \n",
       "\n",
       "[7008 rows x 41 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "comb_raw = pd.read_csv(\"SeqCombinedData.csv\")\n",
    "display(comb_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df0b7bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_raw_filt= comb_raw[[\"Usage\", \"LaggingCurrentReactivePower\", \"LeadingCurrentReactivePower\",\n",
    "                          \"CO2\", \"LaggingCurrentPowerFactor\", \"LeadingCurrentPowerFactor\", \"NSM\",\n",
    "                          \"WeekStatus\", \"Hours\", \"Minutes\", \"IsHoliday\", \"Season\", \"Temp\", \"Dew\",\n",
    "                          \"Humidity\", \"Precip\", \"PrecipProb\", \"PrecipCover\", \"PrecipType\", \"SnowDepth\", \"WindGust\",\n",
    "                          \"WindSpeed\", \"WindDir\", \"SeaLevelPressure\", \"CloudCover\", \"Visibility\", \"SolarRadiation\",\n",
    "                          \"SolarEnergy\", \"UvIndex\", \"Conditions\", \"SunriseHour\", \"LoadType\", \"SunsetHour\"]]\n",
    "load_values = comb_raw.pop(\"LoadType\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fddc0ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "steel_cols_to_encode = ['IsHoliday','PrecipType','Season','Conditions','WeekStatus']\n",
    "steel_cols_to_normalize = ['Usage', 'CO2', 'LaggingCurrentPowerFactor', 'PrecipCover', 'SnowDepth',\n",
    "                          'NSM', 'Hours', 'Minutes', 'PrecipProb', 'SunriseHour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bf48493",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_raw_enc = pd.get_dummies(comb_raw_filt, columns=steel_cols_to_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7a71f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "steel_scaler = StandardScaler()\n",
    "comb_raw_enc[steel_cols_to_normalize] = steel_scaler.fit_transform(comb_raw_enc[steel_cols_to_normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a8632f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Usage</th>\n",
       "      <th>LeadingCurrentReactivePower</th>\n",
       "      <th>LaggingCurrentPowerFactor</th>\n",
       "      <th>LeadingCurrentPowerFactor</th>\n",
       "      <th>NSM</th>\n",
       "      <th>Hours</th>\n",
       "      <th>WindDir</th>\n",
       "      <th>SunriseHour</th>\n",
       "      <th>SunsetHour</th>\n",
       "      <th>IsHoliday_0</th>\n",
       "      <th>IsHoliday_1</th>\n",
       "      <th>PrecipType_rain,snow</th>\n",
       "      <th>PrecipType_snow</th>\n",
       "      <th>Season_Autumn</th>\n",
       "      <th>Season_Spring</th>\n",
       "      <th>Conditions_Rain</th>\n",
       "      <th>Conditions_Snow, Partially cloudy</th>\n",
       "      <th>Conditions_Snow, Rain</th>\n",
       "      <th>Conditions_Snow, Rain, Overcast</th>\n",
       "      <th>WeekStatus_Weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.722530</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.388131</td>\n",
       "      <td>100.00</td>\n",
       "      <td>-1.678015</td>\n",
       "      <td>-1.661325</td>\n",
       "      <td>335.8</td>\n",
       "      <td>1.318323</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.719255</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.675669</td>\n",
       "      <td>100.00</td>\n",
       "      <td>-1.497584</td>\n",
       "      <td>-1.516862</td>\n",
       "      <td>335.8</td>\n",
       "      <td>1.318323</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.713895</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.813898</td>\n",
       "      <td>100.00</td>\n",
       "      <td>-1.317152</td>\n",
       "      <td>-1.372399</td>\n",
       "      <td>335.8</td>\n",
       "      <td>1.318323</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.701091</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1.011218</td>\n",
       "      <td>100.00</td>\n",
       "      <td>-1.136720</td>\n",
       "      <td>-1.083473</td>\n",
       "      <td>335.8</td>\n",
       "      <td>1.318323</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.710917</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.776967</td>\n",
       "      <td>100.00</td>\n",
       "      <td>-0.956288</td>\n",
       "      <td>-0.939010</td>\n",
       "      <td>335.8</td>\n",
       "      <td>1.318323</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7003</th>\n",
       "      <td>-0.715086</td>\n",
       "      <td>9.79</td>\n",
       "      <td>1.025289</td>\n",
       "      <td>32.98</td>\n",
       "      <td>0.884116</td>\n",
       "      <td>0.939010</td>\n",
       "      <td>335.6</td>\n",
       "      <td>1.318323</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7004</th>\n",
       "      <td>-0.699006</td>\n",
       "      <td>18.29</td>\n",
       "      <td>1.025289</td>\n",
       "      <td>21.16</td>\n",
       "      <td>1.064547</td>\n",
       "      <td>1.083473</td>\n",
       "      <td>335.6</td>\n",
       "      <td>1.318323</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7005</th>\n",
       "      <td>-0.716277</td>\n",
       "      <td>13.43</td>\n",
       "      <td>1.025289</td>\n",
       "      <td>24.41</td>\n",
       "      <td>1.244979</td>\n",
       "      <td>1.227936</td>\n",
       "      <td>335.6</td>\n",
       "      <td>1.318323</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7006</th>\n",
       "      <td>-0.715086</td>\n",
       "      <td>13.36</td>\n",
       "      <td>1.025289</td>\n",
       "      <td>24.80</td>\n",
       "      <td>1.425411</td>\n",
       "      <td>1.372399</td>\n",
       "      <td>335.6</td>\n",
       "      <td>1.318323</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7007</th>\n",
       "      <td>-0.702282</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.974286</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.605843</td>\n",
       "      <td>1.661325</td>\n",
       "      <td>335.6</td>\n",
       "      <td>1.318323</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7008 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Usage  LeadingCurrentReactivePower  LaggingCurrentPowerFactor  \\\n",
       "0    -0.722530                         0.00                  -0.388131   \n",
       "1    -0.719255                         0.00                  -0.675669   \n",
       "2    -0.713895                         0.00                  -0.813898   \n",
       "3    -0.701091                         0.00                  -1.011218   \n",
       "4    -0.710917                         0.00                  -0.776967   \n",
       "...        ...                          ...                        ...   \n",
       "7003 -0.715086                         9.79                   1.025289   \n",
       "7004 -0.699006                        18.29                   1.025289   \n",
       "7005 -0.716277                        13.43                   1.025289   \n",
       "7006 -0.715086                        13.36                   1.025289   \n",
       "7007 -0.702282                         0.00                  -0.974286   \n",
       "\n",
       "      LeadingCurrentPowerFactor       NSM     Hours  WindDir  SunriseHour  \\\n",
       "0                        100.00 -1.678015 -1.661325    335.8     1.318323   \n",
       "1                        100.00 -1.497584 -1.516862    335.8     1.318323   \n",
       "2                        100.00 -1.317152 -1.372399    335.8     1.318323   \n",
       "3                        100.00 -1.136720 -1.083473    335.8     1.318323   \n",
       "4                        100.00 -0.956288 -0.939010    335.8     1.318323   \n",
       "...                         ...       ...       ...      ...          ...   \n",
       "7003                      32.98  0.884116  0.939010    335.6     1.318323   \n",
       "7004                      21.16  1.064547  1.083473    335.6     1.318323   \n",
       "7005                      24.41  1.244979  1.227936    335.6     1.318323   \n",
       "7006                      24.80  1.425411  1.372399    335.6     1.318323   \n",
       "7007                     100.00  1.605843  1.661325    335.6     1.318323   \n",
       "\n",
       "      SunsetHour  IsHoliday_0  IsHoliday_1  PrecipType_rain,snow  \\\n",
       "0             17            0            1                     0   \n",
       "1             17            0            1                     0   \n",
       "2             17            0            1                     0   \n",
       "3             17            0            1                     0   \n",
       "4             17            0            1                     0   \n",
       "...          ...          ...          ...                   ...   \n",
       "7003          17            1            0                     0   \n",
       "7004          17            1            0                     0   \n",
       "7005          17            1            0                     0   \n",
       "7006          17            1            0                     0   \n",
       "7007          17            1            0                     0   \n",
       "\n",
       "      PrecipType_snow  Season_Autumn  Season_Spring  Conditions_Rain  \\\n",
       "0                   0              0              0                0   \n",
       "1                   0              0              0                0   \n",
       "2                   0              0              0                0   \n",
       "3                   0              0              0                0   \n",
       "4                   0              0              0                0   \n",
       "...               ...            ...            ...              ...   \n",
       "7003                0              0              0                0   \n",
       "7004                0              0              0                0   \n",
       "7005                0              0              0                0   \n",
       "7006                0              0              0                0   \n",
       "7007                0              0              0                0   \n",
       "\n",
       "      Conditions_Snow, Partially cloudy  Conditions_Snow, Rain  \\\n",
       "0                                     0                      0   \n",
       "1                                     0                      0   \n",
       "2                                     0                      0   \n",
       "3                                     0                      0   \n",
       "4                                     0                      0   \n",
       "...                                 ...                    ...   \n",
       "7003                                  0                      0   \n",
       "7004                                  0                      0   \n",
       "7005                                  0                      0   \n",
       "7006                                  0                      0   \n",
       "7007                                  0                      0   \n",
       "\n",
       "      Conditions_Snow, Rain, Overcast  WeekStatus_Weekday  \n",
       "0                                   0                   1  \n",
       "1                                   0                   1  \n",
       "2                                   0                   1  \n",
       "3                                   0                   1  \n",
       "4                                   0                   1  \n",
       "...                               ...                 ...  \n",
       "7003                                0                   1  \n",
       "7004                                0                   1  \n",
       "7005                                0                   1  \n",
       "7006                                0                   1  \n",
       "7007                                0                   1  \n",
       "\n",
       "[7008 rows x 20 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_selection_columns = [\n",
    "    'Usage', 'LeadingCurrentReactivePower',\n",
    "       'LaggingCurrentPowerFactor', 'LeadingCurrentPowerFactor', 'NSM',\n",
    "       'Hours', 'WindDir', 'SunriseHour', 'SunsetHour', 'IsHoliday_0',\n",
    "       'IsHoliday_1', 'PrecipType_rain,snow', 'PrecipType_snow',\n",
    "       'Season_Autumn', 'Season_Spring', 'Conditions_Rain',\n",
    "       'Conditions_Snow, Partially cloudy', 'Conditions_Snow, Rain',\n",
    "       'Conditions_Snow, Rain, Overcast', 'WeekStatus_Weekday'\n",
    "        ]\n",
    "comb_raw_filt = comb_raw_enc[feature_selection_columns]\n",
    "display(comb_raw_filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13a6960b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_comb = []\n",
    "for num in range(4,len(comb_raw_filt)):\n",
    "    ele =comb_raw_filt[num-4:num+1]\n",
    "    transformed_comb.append(ele)\n",
    "transformed_comb = np.array(transformed_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29b4bf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7004\n"
     ]
    }
   ],
   "source": [
    "X_arr = transformed_comb\n",
    "Y_arr = load_values[4:]\n",
    "print(len(Y_arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d5e6e0",
   "metadata": {},
   "source": [
    "### Pruning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb89341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_prune_dense_layer(k_weights1,k_weights2, b_weights, k_sparsity):\n",
    "    # Copy the kernel weights and get ranked indeces of the abs\n",
    "    kernel_weights1 = np.copy(k_weights1)\n",
    "    ind1 = np.unravel_index(\n",
    "        np.argsort(\n",
    "            np.abs(kernel_weights1),\n",
    "            axis=None),\n",
    "        kernel_weights1.shape)\n",
    "    \n",
    "    kernel_weights2 = np.copy(k_weights2)\n",
    "    ind2 = np.unravel_index(\n",
    "        np.argsort(\n",
    "            np.abs(kernel_weights2),\n",
    "            axis=None),\n",
    "        kernel_weights2.shape)\n",
    "    \n",
    "    \n",
    "    # Number of indexes to set to 0\n",
    "    cutoff1 = int(len(ind1[0])*k_sparsity)\n",
    "    # The indexes in the 2D kernel weight matrix to set to 0\n",
    "    sparse_cutoff_inds1 = (ind1[0][0:cutoff1], ind1[1][0:cutoff1])\n",
    "    kernel_weights1[sparse_cutoff_inds1] = 0.\n",
    "    \n",
    "    cutoff2 = int(len(ind2[0])*k_sparsity)\n",
    "    # The indexes in the 2D kernel weight matrix to set to 0\n",
    "    sparse_cutoff_inds2 = (ind2[0][0:cutoff2], ind2[1][0:cutoff2])\n",
    "    kernel_weights2[sparse_cutoff_inds2] = 0.\n",
    "        \n",
    "    # Copy the bias weights and get ranked indeces of the abs\n",
    "    bias_weights = np.copy(b_weights)\n",
    "    ind = np.unravel_index(\n",
    "        np.argsort(\n",
    "            np.abs(bias_weights), \n",
    "            axis=None), \n",
    "        bias_weights.shape)\n",
    "        \n",
    "    # Number of indexes to set to 0\n",
    "    cutoff = int(len(ind[0])*k_sparsity)\n",
    "    # The indexes in the 1D bias weight matrix to set to 0\n",
    "    sparse_cutoff_inds = (ind[0][0:cutoff])\n",
    "    bias_weights[sparse_cutoff_inds] = 0.\n",
    "    \n",
    "    return kernel_weights1, kernel_weights2, bias_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "921231af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparsify_model(model, x_train, y_train, x_test, y_test, k_sparsity, pruning='weight'):\n",
    "\n",
    "    # Copying a temporary sparse model from our original\n",
    "    sparse_model = tf.keras.models.clone_model(model)\n",
    "    sparse_model.set_weights(model.get_weights())\n",
    "    \n",
    "    # Getting a list of the names of each component (w + b) of each layer\n",
    "    names = [weight.name for layer in sparse_model.layers for weight in layer.weights]\n",
    "    # Getting the list of the weights for each component (w + b) of each layer\n",
    "    weights = sparse_model.get_weights()\n",
    "    \n",
    "    # Initializing list that will contain the new sparse weights\n",
    "    newWeightList = []\n",
    "\n",
    "    # Iterate over all but the final 2 layers (the softmax)\n",
    "    for i in range(0, len(weights)-2, 3):\n",
    "        kernel_weights1,kernel_weights2, bias_weights = weight_prune_dense_layer(weights[i],\n",
    "                                                                    weights[i+1],\n",
    "                                                                    weights[i+2],\n",
    "                                                                    k_sparsity)\n",
    "        # Append the new weight list with our sparsified kernel weights\n",
    "        newWeightList.append(kernel_weights1)\n",
    "        newWeightList.append(kernel_weights2)\n",
    "        \n",
    "        # Append the new weight list with our sparsified bias weights\n",
    "        newWeightList.append(bias_weights)\n",
    "\n",
    "    # Adding the unchanged weights of the final 2 layers\n",
    "    for i in range(len(weights)-2, len(weights)):\n",
    "        unmodified_weight = np.copy(weights[i])\n",
    "        newWeightList.append(unmodified_weight)\n",
    "\n",
    "    # Setting the weights of our model to the new ones\n",
    "    sparse_model.set_weights(newWeightList)\n",
    "    \n",
    "    # Re-compiling the Keras model (necessary for using `evaluate()`)\n",
    "    sparse_model.compile(\n",
    "        loss=tf.keras.losses.categorical_crossentropy,\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy',\n",
    "                 tf.keras.metrics.Precision(),\n",
    "                 tfa.metrics.F1Score(num_classes=3, average='macro', threshold=0.5)\n",
    "                ])\n",
    "    \n",
    "    # Printing the the associated loss & Accuracy for the k% sparsity\n",
    "    score = sparse_model.evaluate(x_train, y_train, verbose=0)\n",
    "    score_t = sparse_model.evaluate(x_test, y_test, verbose=0)\n",
    "    \n",
    "    print('k% weight sparsity: ', k_sparsity,\n",
    "          #'\\tTrain loss: {:07.5f}'.format(score[0]),\n",
    "          #'\\tTrain accuracy: {:05.2f} %'.format(score[1]*100.),\n",
    "          '\\tTest loss: {:07.5f}'.format(score_t[0]),\n",
    "          '\\tTest accuracy: {:05.2f} %'.format(score_t[1]*100.),\n",
    "          '\\tTest precision: {:05.2f} %'.format(score_t[2]*100.),\n",
    "          '\\tTest F1 score: {:05.2f} %'.format(score_t[3]*100.),\n",
    "         )\n",
    "    \n",
    "    return sparse_model, score_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71536316",
   "metadata": {},
   "source": [
    "### Metrics of models over different folds of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ffa35af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "176/176 [==============================] - 9s 10ms/step - loss: 0.9687 - accuracy: 0.5153 - precision_80: 0.6628 - f1_score: 0.2171\n",
      "Epoch 2/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.6007 - accuracy: 0.7246 - precision_80: 0.8072 - f1_score: 0.5190\n",
      "Epoch 3/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.4656 - accuracy: 0.7719 - precision_80: 0.8009 - f1_score: 0.6748\n",
      "Epoch 4/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.4219 - accuracy: 0.7967 - precision_80: 0.8143 - f1_score: 0.7122\n",
      "Epoch 5/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.3916 - accuracy: 0.8215 - precision_80: 0.8376 - f1_score: 0.7559\n",
      "Epoch 6/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.3842 - accuracy: 0.8251 - precision_80: 0.8362 - f1_score: 0.7658\n",
      "Epoch 7/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.3631 - accuracy: 0.8317 - precision_80: 0.8450 - f1_score: 0.7784\n",
      "Epoch 8/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.3612 - accuracy: 0.8333 - precision_80: 0.8452 - f1_score: 0.7819\n",
      "Epoch 9/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.3521 - accuracy: 0.8372 - precision_80: 0.8482 - f1_score: 0.7835\n",
      "Epoch 10/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.3458 - accuracy: 0.8356 - precision_80: 0.8500 - f1_score: 0.7866\n",
      "Epoch 11/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.3467 - accuracy: 0.8367 - precision_80: 0.8450 - f1_score: 0.7834\n",
      "Epoch 12/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.3326 - accuracy: 0.8462 - precision_80: 0.8576 - f1_score: 0.7974\n",
      "Epoch 13/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.3331 - accuracy: 0.8415 - precision_80: 0.8538 - f1_score: 0.7924\n",
      "Epoch 14/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.3255 - accuracy: 0.8476 - precision_80: 0.8600 - f1_score: 0.8004\n",
      "Epoch 15/20\n",
      "176/176 [==============================] - 2s 9ms/step - loss: 0.3299 - accuracy: 0.8451 - precision_80: 0.8584 - f1_score: 0.7975\n",
      "Epoch 16/20\n",
      "176/176 [==============================] - 2s 9ms/step - loss: 0.3213 - accuracy: 0.8472 - precision_80: 0.8591 - f1_score: 0.8005\n",
      "Epoch 17/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.3145 - accuracy: 0.8526 - precision_80: 0.8643 - f1_score: 0.8052\n",
      "Epoch 18/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.3111 - accuracy: 0.8563 - precision_80: 0.8670 - f1_score: 0.8092\n",
      "Epoch 19/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.3071 - accuracy: 0.8558 - precision_80: 0.8661 - f1_score: 0.8103\n",
      "Epoch 20/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.3139 - accuracy: 0.8499 - precision_80: 0.8596 - f1_score: 0.8001\n",
      "k% weight sparsity:  0.0 \tTest loss: 0.30260 \tTest accuracy: 86.58 % \tTest precision: 87.76 % \tTest F1 score: 82.13 %\n",
      "k% weight sparsity:  0.05 \tTest loss: 0.50092 \tTest accuracy: 78.09 % \tTest precision: 78.47 % \tTest F1 score: 73.03 %\n",
      "k% weight sparsity:  0.1 \tTest loss: 1.48243 \tTest accuracy: 63.81 % \tTest precision: 65.13 % \tTest F1 score: 47.33 %\n",
      "k% weight sparsity:  0.15 \tTest loss: 1.62954 \tTest accuracy: 60.10 % \tTest precision: 61.76 % \tTest F1 score: 42.33 %\n",
      "k% weight sparsity:  0.2 \tTest loss: 1.99146 \tTest accuracy: 59.46 % \tTest precision: 60.52 % \tTest F1 score: 37.79 %\n",
      "k% weight sparsity:  0.25 \tTest loss: 3.39025 \tTest accuracy: 54.46 % \tTest precision: 54.58 % \tTest F1 score: 25.44 %\n",
      "k% weight sparsity:  0.3 \tTest loss: 1.16930 \tTest accuracy: 67.74 % \tTest precision: 68.98 % \tTest F1 score: 49.22 %\n",
      "k% weight sparsity:  0.35 \tTest loss: 3.06332 \tTest accuracy: 53.89 % \tTest precision: 54.12 % \tTest F1 score: 24.67 %\n",
      "k% weight sparsity:  0.4 \tTest loss: 2.46981 \tTest accuracy: 53.96 % \tTest precision: 54.09 % \tTest F1 score: 24.67 %\n",
      "Epoch 1/20\n",
      "176/176 [==============================] - 10s 12ms/step - loss: 1.0220 - accuracy: 0.5117 - precision_90: 0.5307 - f1_score: 0.1939\n",
      "Epoch 2/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.8831 - accuracy: 0.5743 - precision_90: 0.6998 - f1_score: 0.2813\n",
      "Epoch 3/20\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 0.6206 - accuracy: 0.7303 - precision_90: 0.8287 - f1_score: 0.5195\n",
      "Epoch 4/20\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 0.5002 - accuracy: 0.7990 - precision_90: 0.8441 - f1_score: 0.6930\n",
      "Epoch 5/20\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 0.4226 - accuracy: 0.8185 - precision_90: 0.8498 - f1_score: 0.7581\n",
      "Epoch 6/20\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 0.3830 - accuracy: 0.8369 - precision_90: 0.8533 - f1_score: 0.7816\n",
      "Epoch 7/20\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 0.3631 - accuracy: 0.8420 - precision_90: 0.8579 - f1_score: 0.7900\n",
      "Epoch 8/20\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 0.3477 - accuracy: 0.8472 - precision_90: 0.8625 - f1_score: 0.7956\n",
      "Epoch 9/20\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 0.3314 - accuracy: 0.8510 - precision_90: 0.8632 - f1_score: 0.8032\n",
      "Epoch 10/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.3324 - accuracy: 0.8513 - precision_90: 0.8621 - f1_score: 0.8060\n",
      "Epoch 11/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.3228 - accuracy: 0.8547 - precision_90: 0.8653 - f1_score: 0.8081\n",
      "Epoch 12/20\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 0.3125 - accuracy: 0.8570 - precision_90: 0.8686 - f1_score: 0.8155\n",
      "Epoch 13/20\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 0.3044 - accuracy: 0.8617 - precision_90: 0.8716 - f1_score: 0.8169\n",
      "Epoch 14/20\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 0.3036 - accuracy: 0.8672 - precision_90: 0.8747 - f1_score: 0.8264\n",
      "Epoch 15/20\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 0.2911 - accuracy: 0.8706 - precision_90: 0.8799 - f1_score: 0.8307\n",
      "Epoch 16/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.2866 - accuracy: 0.8710 - precision_90: 0.8829 - f1_score: 0.8324\n",
      "Epoch 17/20\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 0.2786 - accuracy: 0.8752 - precision_90: 0.8856 - f1_score: 0.8387\n",
      "Epoch 18/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.2707 - accuracy: 0.8802 - precision_90: 0.8900 - f1_score: 0.8428\n",
      "Epoch 19/20\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 0.2594 - accuracy: 0.8858 - precision_90: 0.8966 - f1_score: 0.8505\n",
      "Epoch 20/20\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 0.2531 - accuracy: 0.8868 - precision_90: 0.8957 - f1_score: 0.8517\n",
      "k% weight sparsity:  0.0 \tTest loss: 0.26774 \tTest accuracy: 87.15 % \tTest precision: 88.16 % \tTest F1 score: 83.00 %\n",
      "k% weight sparsity:  0.05 \tTest loss: 0.27572 \tTest accuracy: 87.44 % \tTest precision: 88.83 % \tTest F1 score: 84.80 %\n",
      "k% weight sparsity:  0.1 \tTest loss: 0.30156 \tTest accuracy: 84.80 % \tTest precision: 85.91 % \tTest F1 score: 80.48 %\n",
      "k% weight sparsity:  0.15 \tTest loss: 0.34547 \tTest accuracy: 82.80 % \tTest precision: 84.45 % \tTest F1 score: 77.46 %\n",
      "k% weight sparsity:  0.2 \tTest loss: 0.35085 \tTest accuracy: 82.87 % \tTest precision: 84.06 % \tTest F1 score: 76.80 %\n",
      "k% weight sparsity:  0.25 \tTest loss: 0.43941 \tTest accuracy: 79.87 % \tTest precision: 81.63 % \tTest F1 score: 69.00 %\n",
      "k% weight sparsity:  0.3 \tTest loss: 4.24794 \tTest accuracy: 51.53 % \tTest precision: 51.53 % \tTest F1 score: 22.83 %\n",
      "k% weight sparsity:  0.35 \tTest loss: 4.41081 \tTest accuracy: 51.46 % \tTest precision: 51.46 % \tTest F1 score: 22.65 %\n",
      "k% weight sparsity:  0.4 \tTest loss: 4.44543 \tTest accuracy: 51.53 % \tTest precision: 51.53 % \tTest F1 score: 22.83 %\n",
      "Epoch 1/20\n",
      "176/176 [==============================] - 10s 11ms/step - loss: 0.9507 - accuracy: 0.5451 - precision_100: 0.7114 - f1_score: 0.2586\n",
      "Epoch 2/20\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 0.7038 - accuracy: 0.6905 - precision_100: 0.7923 - f1_score: 0.4687\n",
      "Epoch 3/20\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 0.5675 - accuracy: 0.7587 - precision_100: 0.8408 - f1_score: 0.5741\n",
      "Epoch 4/20\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 0.4718 - accuracy: 0.8014 - precision_100: 0.8470 - f1_score: 0.7159\n",
      "Epoch 5/20\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 0.4138 - accuracy: 0.8292 - precision_100: 0.8582 - f1_score: 0.7655\n",
      "Epoch 6/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.3901 - accuracy: 0.8394 - precision_100: 0.8601 - f1_score: 0.7795\n",
      "Epoch 7/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.3660 - accuracy: 0.8403 - precision_100: 0.8561 - f1_score: 0.7873\n",
      "Epoch 8/20\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 0.3494 - accuracy: 0.8460 - precision_100: 0.8602 - f1_score: 0.7953\n",
      "Epoch 9/20\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 0.3327 - accuracy: 0.8503 - precision_100: 0.8645 - f1_score: 0.8029\n",
      "Epoch 10/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.3224 - accuracy: 0.8569 - precision_100: 0.8688 - f1_score: 0.8140\n",
      "Epoch 11/20\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 0.3129 - accuracy: 0.8588 - precision_100: 0.8712 - f1_score: 0.8141\n",
      "Epoch 12/20\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 0.2996 - accuracy: 0.8649 - precision_100: 0.8751 - f1_score: 0.8255\n",
      "Epoch 13/20\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 0.2883 - accuracy: 0.8758 - precision_100: 0.8838 - f1_score: 0.8391\n",
      "Epoch 14/20\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 0.2814 - accuracy: 0.8783 - precision_100: 0.8892 - f1_score: 0.8411\n",
      "Epoch 15/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.2725 - accuracy: 0.8785 - precision_100: 0.8885 - f1_score: 0.8419\n",
      "Epoch 16/20\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 0.2580 - accuracy: 0.8881 - precision_100: 0.8997 - f1_score: 0.8568\n",
      "Epoch 17/20\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 0.2537 - accuracy: 0.8856 - precision_100: 0.8976 - f1_score: 0.8540\n",
      "Epoch 18/20\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 0.2415 - accuracy: 0.8981 - precision_100: 0.9105 - f1_score: 0.8716\n",
      "Epoch 19/20\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 0.2375 - accuracy: 0.8990 - precision_100: 0.9094 - f1_score: 0.8712\n",
      "Epoch 20/20\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 0.2380 - accuracy: 0.8945 - precision_100: 0.9032 - f1_score: 0.8632\n",
      "k% weight sparsity:  0.0 \tTest loss: 0.22312 \tTest accuracy: 91.36 % \tTest precision: 92.08 % \tTest F1 score: 88.91 %\n",
      "k% weight sparsity:  0.05 \tTest loss: 0.27382 \tTest accuracy: 88.22 % \tTest precision: 89.71 % \tTest F1 score: 85.65 %\n",
      "k% weight sparsity:  0.1 \tTest loss: 0.34279 \tTest accuracy: 85.80 % \tTest precision: 87.30 % \tTest F1 score: 83.47 %\n",
      "k% weight sparsity:  0.15 \tTest loss: 0.42655 \tTest accuracy: 82.94 % \tTest precision: 83.75 % \tTest F1 score: 77.59 %\n",
      "k% weight sparsity:  0.2 \tTest loss: 3.24428 \tTest accuracy: 52.46 % \tTest precision: 52.47 % \tTest F1 score: 23.71 %\n",
      "k% weight sparsity:  0.25 \tTest loss: 3.00633 \tTest accuracy: 52.53 % \tTest precision: 52.80 % \tTest F1 score: 24.19 %\n",
      "k% weight sparsity:  0.3 \tTest loss: 2.97536 \tTest accuracy: 53.10 % \tTest precision: 53.51 % \tTest F1 score: 25.21 %\n",
      "k% weight sparsity:  0.35 \tTest loss: 3.80666 \tTest accuracy: 52.25 % \tTest precision: 52.29 % \tTest F1 score: 23.41 %\n",
      "k% weight sparsity:  0.4 \tTest loss: 3.49622 \tTest accuracy: 52.39 % \tTest precision: 52.51 % \tTest F1 score: 23.47 %\n",
      "Epoch 1/20\n",
      "176/176 [==============================] - 9s 10ms/step - loss: 0.9813 - accuracy: 0.5153 - precision_110: 0.6183 - f1_score: 0.2225\n",
      "Epoch 2/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.6631 - accuracy: 0.6941 - precision_110: 0.8000 - f1_score: 0.4644\n",
      "Epoch 3/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.4797 - accuracy: 0.7869 - precision_110: 0.8306 - f1_score: 0.6477\n",
      "Epoch 4/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.4120 - accuracy: 0.8064 - precision_110: 0.8361 - f1_score: 0.7273\n",
      "Epoch 5/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.3780 - accuracy: 0.8283 - precision_110: 0.8453 - f1_score: 0.7676\n",
      "Epoch 6/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.3467 - accuracy: 0.8467 - precision_110: 0.8562 - f1_score: 0.7941\n",
      "Epoch 7/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.3355 - accuracy: 0.8487 - precision_110: 0.8626 - f1_score: 0.8009\n",
      "Epoch 8/20\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 0.3325 - accuracy: 0.8478 - precision_110: 0.8566 - f1_score: 0.7989\n",
      "Epoch 9/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.3184 - accuracy: 0.8561 - precision_110: 0.8665 - f1_score: 0.8096\n",
      "Epoch 10/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.3107 - accuracy: 0.8599 - precision_110: 0.8671 - f1_score: 0.8130\n",
      "Epoch 11/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.2970 - accuracy: 0.8663 - precision_110: 0.8750 - f1_score: 0.8229\n",
      "Epoch 12/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.2945 - accuracy: 0.8660 - precision_110: 0.8754 - f1_score: 0.8235\n",
      "Epoch 13/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.2857 - accuracy: 0.8701 - precision_110: 0.8801 - f1_score: 0.8277\n",
      "Epoch 14/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.2826 - accuracy: 0.8756 - precision_110: 0.8848 - f1_score: 0.8356\n",
      "Epoch 15/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.2688 - accuracy: 0.8838 - precision_110: 0.8903 - f1_score: 0.8461\n",
      "Epoch 16/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.2615 - accuracy: 0.8810 - precision_110: 0.8901 - f1_score: 0.8428\n",
      "Epoch 17/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.2447 - accuracy: 0.8926 - precision_110: 0.9007 - f1_score: 0.8584\n",
      "Epoch 18/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.2465 - accuracy: 0.8961 - precision_110: 0.9056 - f1_score: 0.8651\n",
      "Epoch 19/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.2311 - accuracy: 0.9045 - precision_110: 0.9114 - f1_score: 0.8771\n",
      "Epoch 20/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.2314 - accuracy: 0.9001 - precision_110: 0.9083 - f1_score: 0.8688\n",
      "k% weight sparsity:  0.0 \tTest loss: 0.25797 \tTest accuracy: 88.51 % \tTest precision: 89.25 % \tTest F1 score: 85.28 %\n",
      "k% weight sparsity:  0.05 \tTest loss: 0.24379 \tTest accuracy: 89.58 % \tTest precision: 90.25 % \tTest F1 score: 86.56 %\n",
      "k% weight sparsity:  0.1 \tTest loss: 1.16493 \tTest accuracy: 66.10 % \tTest precision: 67.01 % \tTest F1 score: 54.74 %\n",
      "k% weight sparsity:  0.15 \tTest loss: 0.85505 \tTest accuracy: 67.02 % \tTest precision: 68.49 % \tTest F1 score: 56.59 %\n",
      "k% weight sparsity:  0.2 \tTest loss: 0.97211 \tTest accuracy: 65.67 % \tTest precision: 66.87 % \tTest F1 score: 53.82 %\n",
      "k% weight sparsity:  0.25 \tTest loss: 0.62748 \tTest accuracy: 73.95 % \tTest precision: 76.63 % \tTest F1 score: 71.51 %\n",
      "k% weight sparsity:  0.3 \tTest loss: 0.85004 \tTest accuracy: 71.95 % \tTest precision: 72.39 % \tTest F1 score: 64.06 %\n",
      "k% weight sparsity:  0.35 \tTest loss: 1.15126 \tTest accuracy: 69.59 % \tTest precision: 69.86 % \tTest F1 score: 56.70 %\n",
      "k% weight sparsity:  0.4 \tTest loss: 1.10658 \tTest accuracy: 67.88 % \tTest precision: 68.18 % \tTest F1 score: 49.62 %\n",
      "Epoch 1/20\n",
      "176/176 [==============================] - 9s 10ms/step - loss: 0.9724 - accuracy: 0.5249 - precision_120: 0.6707 - f1_score: 0.1980\n",
      "Epoch 2/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.7147 - accuracy: 0.6786 - precision_120: 0.8058 - f1_score: 0.4242\n",
      "Epoch 3/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.5269 - accuracy: 0.7624 - precision_120: 0.8455 - f1_score: 0.5699\n",
      "Epoch 4/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.4314 - accuracy: 0.8110 - precision_120: 0.8541 - f1_score: 0.7156\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176/176 [==============================] - 2s 10ms/step - loss: 0.3820 - accuracy: 0.8324 - precision_120: 0.8537 - f1_score: 0.7637\n",
      "Epoch 6/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.3563 - accuracy: 0.8451 - precision_120: 0.8595 - f1_score: 0.7910\n",
      "Epoch 7/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.3417 - accuracy: 0.8479 - precision_120: 0.8602 - f1_score: 0.7973\n",
      "Epoch 8/20\n",
      "176/176 [==============================] - 2s 9ms/step - loss: 0.3301 - accuracy: 0.8528 - precision_120: 0.8635 - f1_score: 0.8038\n",
      "Epoch 9/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.3138 - accuracy: 0.8579 - precision_120: 0.8665 - f1_score: 0.8100\n",
      "Epoch 10/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.3147 - accuracy: 0.8553 - precision_120: 0.8661 - f1_score: 0.8108\n",
      "Epoch 11/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.2999 - accuracy: 0.8667 - precision_120: 0.8753 - f1_score: 0.8254\n",
      "Epoch 12/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.2940 - accuracy: 0.8658 - precision_120: 0.8738 - f1_score: 0.8231\n",
      "Epoch 13/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.2822 - accuracy: 0.8701 - precision_120: 0.8799 - f1_score: 0.8302\n",
      "Epoch 14/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.2876 - accuracy: 0.8683 - precision_120: 0.8737 - f1_score: 0.8253\n",
      "Epoch 15/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.2718 - accuracy: 0.8756 - precision_120: 0.8800 - f1_score: 0.8357\n",
      "Epoch 16/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.2630 - accuracy: 0.8810 - precision_120: 0.8879 - f1_score: 0.8444\n",
      "Epoch 17/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.2591 - accuracy: 0.8808 - precision_120: 0.8895 - f1_score: 0.8456\n",
      "Epoch 18/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.2495 - accuracy: 0.8835 - precision_120: 0.8902 - f1_score: 0.8475\n",
      "Epoch 19/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.2439 - accuracy: 0.8852 - precision_120: 0.8902 - f1_score: 0.8495\n",
      "Epoch 20/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.2324 - accuracy: 0.8968 - precision_120: 0.9022 - f1_score: 0.8643\n",
      "k% weight sparsity:  0.0 \tTest loss: 0.25622 \tTest accuracy: 87.72 % \tTest precision: 87.90 % \tTest F1 score: 84.48 %\n",
      "k% weight sparsity:  0.05 \tTest loss: 0.26733 \tTest accuracy: 86.80 % \tTest precision: 87.39 % \tTest F1 score: 83.97 %\n",
      "k% weight sparsity:  0.1 \tTest loss: 0.28743 \tTest accuracy: 86.72 % \tTest precision: 87.12 % \tTest F1 score: 83.25 %\n",
      "k% weight sparsity:  0.15 \tTest loss: 0.29450 \tTest accuracy: 86.58 % \tTest precision: 87.46 % \tTest F1 score: 83.17 %\n",
      "k% weight sparsity:  0.2 \tTest loss: 0.38952 \tTest accuracy: 85.01 % \tTest precision: 85.98 % \tTest F1 score: 80.82 %\n",
      "k% weight sparsity:  0.25 \tTest loss: 0.40130 \tTest accuracy: 83.87 % \tTest precision: 84.95 % \tTest F1 score: 80.22 %\n",
      "k% weight sparsity:  0.3 \tTest loss: 4.86717 \tTest accuracy: 50.11 % \tTest precision: 50.11 % \tTest F1 score: 22.25 %\n",
      "k% weight sparsity:  0.35 \tTest loss: 5.03140 \tTest accuracy: 50.18 % \tTest precision: 50.18 % \tTest F1 score: 22.44 %\n",
      "k% weight sparsity:  0.4 \tTest loss: 5.22157 \tTest accuracy: 50.18 % \tTest precision: 50.18 % \tTest F1 score: 22.46 %\n",
      "Epoch 1/20\n",
      "176/176 [==============================] - 10s 11ms/step - loss: 0.9996 - accuracy: 0.5021 - precision_130: 0.5843 - f1_score: 0.2050\n",
      "Epoch 2/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.6486 - accuracy: 0.7103 - precision_130: 0.8221 - f1_score: 0.4943\n",
      "Epoch 3/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.4851 - accuracy: 0.7926 - precision_130: 0.8572 - f1_score: 0.6650\n",
      "Epoch 4/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.4282 - accuracy: 0.8149 - precision_130: 0.8517 - f1_score: 0.7378\n",
      "Epoch 5/20\n",
      "176/176 [==============================] - 2s 9ms/step - loss: 0.3927 - accuracy: 0.8308 - precision_130: 0.8556 - f1_score: 0.7641\n",
      "Epoch 6/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.3668 - accuracy: 0.8406 - precision_130: 0.8602 - f1_score: 0.7820\n",
      "Epoch 7/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.3483 - accuracy: 0.8445 - precision_130: 0.8614 - f1_score: 0.7954\n",
      "Epoch 8/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.3398 - accuracy: 0.8520 - precision_130: 0.8651 - f1_score: 0.8001\n",
      "Epoch 9/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.3351 - accuracy: 0.8492 - precision_130: 0.8624 - f1_score: 0.8013\n",
      "Epoch 10/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.3312 - accuracy: 0.8504 - precision_130: 0.8601 - f1_score: 0.8000\n",
      "Epoch 11/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.3145 - accuracy: 0.8556 - precision_130: 0.8692 - f1_score: 0.8122\n",
      "Epoch 12/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.3038 - accuracy: 0.8601 - precision_130: 0.8699 - f1_score: 0.8164\n",
      "Epoch 13/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.3009 - accuracy: 0.8660 - precision_130: 0.8770 - f1_score: 0.8231\n",
      "Epoch 14/20\n",
      "176/176 [==============================] - 2s 9ms/step - loss: 0.2935 - accuracy: 0.8640 - precision_130: 0.8753 - f1_score: 0.8233\n",
      "Epoch 15/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.2880 - accuracy: 0.8688 - precision_130: 0.8786 - f1_score: 0.8262\n",
      "Epoch 16/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.2826 - accuracy: 0.8731 - precision_130: 0.8817 - f1_score: 0.8313\n",
      "Epoch 17/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.2756 - accuracy: 0.8722 - precision_130: 0.8841 - f1_score: 0.8334\n",
      "Epoch 18/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.2638 - accuracy: 0.8818 - precision_130: 0.8910 - f1_score: 0.8458\n",
      "Epoch 19/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.2556 - accuracy: 0.8872 - precision_130: 0.8965 - f1_score: 0.8536\n",
      "Epoch 20/20\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 0.2532 - accuracy: 0.8870 - precision_130: 0.8960 - f1_score: 0.8498\n",
      "k% weight sparsity:  0.0 \tTest loss: 0.28652 \tTest accuracy: 86.94 % \tTest precision: 88.24 % \tTest F1 score: 83.94 %\n",
      "k% weight sparsity:  0.05 \tTest loss: 0.28085 \tTest accuracy: 87.51 % \tTest precision: 88.64 % \tTest F1 score: 84.60 %\n",
      "k% weight sparsity:  0.1 \tTest loss: 0.35393 \tTest accuracy: 79.94 % \tTest precision: 80.73 % \tTest F1 score: 74.22 %\n",
      "k% weight sparsity:  0.15 \tTest loss: 1.00961 \tTest accuracy: 66.60 % \tTest precision: 68.69 % \tTest F1 score: 54.19 %\n",
      "k% weight sparsity:  0.2 \tTest loss: 1.05216 \tTest accuracy: 65.31 % \tTest precision: 67.23 % \tTest F1 score: 50.49 %\n",
      "k% weight sparsity:  0.25 \tTest loss: 1.34978 \tTest accuracy: 61.60 % \tTest precision: 63.05 % \tTest F1 score: 44.89 %\n",
      "k% weight sparsity:  0.3 \tTest loss: 1.44287 \tTest accuracy: 58.82 % \tTest precision: 59.83 % \tTest F1 score: 38.93 %\n",
      "k% weight sparsity:  0.35 \tTest loss: 3.08145 \tTest accuracy: 51.46 % \tTest precision: 51.62 % \tTest F1 score: 25.28 %\n",
      "k% weight sparsity:  0.4 \tTest loss: 1.79015 \tTest accuracy: 56.89 % \tTest precision: 57.44 % \tTest F1 score: 37.25 %\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame()\n",
    "\n",
    "random_seeds = [10, 15, 20, 25, 30, 35]\n",
    "k_sparsities = [0.0, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40]\n",
    "untuned_acc_array = []\n",
    "untuned_precision_array=[]\n",
    "untuned_f1_array=[]\n",
    "\n",
    "count = 1\n",
    "for itr in random_seeds:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_arr, Y_arr, test_size=0.20, random_state=itr)\n",
    "    y_train_le = pd.get_dummies(y_train)\n",
    "    y_test_le = pd.get_dummies(y_test)\n",
    "    \n",
    "    model_1 = tf.keras.models.Sequential()\n",
    "    model_1.add(LSTM(20, return_sequences=True))\n",
    "    model_1.add(LSTM(20, return_sequences=False))\n",
    "    model_1.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    model_1.compile(loss='categorical_crossentropy',\n",
    "              optimizer=\"adam\",\n",
    "              metrics=['accuracy',\n",
    "                      tf.keras.metrics.Precision(),\n",
    "                      tfa.metrics.F1Score(num_classes=3,\n",
    "                                                  average='macro',\n",
    "                                                  threshold=0.5)])\n",
    "    \n",
    "    model_1.fit(x=X_train, y=y_train_le, epochs=20)\n",
    "    \n",
    "    model1_loss_values = []\n",
    "    model1_accs_values = []\n",
    "    model1_prec_values = []\n",
    "    model1_f1_values = []\n",
    "\n",
    "    for k_sparsity in k_sparsities:\n",
    "        sparse_model1, score = sparsify_model(model_1, x_train=X_train,\n",
    "                                         y_train=y_train_le,\n",
    "                                          x_test=X_test,\n",
    "                                          y_test=y_test_le,\n",
    "                                         k_sparsity=k_sparsity, \n",
    "                                         pruning='weight')\n",
    "        model1_loss_values.append(score[0])\n",
    "        model1_accs_values.append(score[1])\n",
    "        model1_prec_values.append(score[2])\n",
    "        model1_f1_values.append(score[3])\n",
    "        \n",
    "    col1_name = \"model\"+str(count)+\"acc\"\n",
    "    col2_name = \"model\"+str(count)+\"prec\"\n",
    "    col3_name = \"model\"+str(count)+\"f1\"\n",
    "    count = count+1\n",
    "        \n",
    "    results_df[col1_name] = model1_accs_values\n",
    "    results_df[col2_name] = model1_prec_values\n",
    "    results_df[col3_name] = model1_f1_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c536be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model1acc</th>\n",
       "      <th>model1prec</th>\n",
       "      <th>model1f1</th>\n",
       "      <th>model2acc</th>\n",
       "      <th>model2prec</th>\n",
       "      <th>model2f1</th>\n",
       "      <th>model3acc</th>\n",
       "      <th>model3prec</th>\n",
       "      <th>model3f1</th>\n",
       "      <th>model4acc</th>\n",
       "      <th>model4prec</th>\n",
       "      <th>model4f1</th>\n",
       "      <th>model5acc</th>\n",
       "      <th>model5prec</th>\n",
       "      <th>model5f1</th>\n",
       "      <th>model6acc</th>\n",
       "      <th>model6prec</th>\n",
       "      <th>model6f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.858469</td>\n",
       "      <td>0.867345</td>\n",
       "      <td>0.815639</td>\n",
       "      <td>0.892736</td>\n",
       "      <td>0.906456</td>\n",
       "      <td>0.861495</td>\n",
       "      <td>0.907728</td>\n",
       "      <td>0.914265</td>\n",
       "      <td>0.884877</td>\n",
       "      <td>0.896306</td>\n",
       "      <td>0.903068</td>\n",
       "      <td>0.863371</td>\n",
       "      <td>0.898804</td>\n",
       "      <td>0.904495</td>\n",
       "      <td>0.869921</td>\n",
       "      <td>0.896484</td>\n",
       "      <td>0.906968</td>\n",
       "      <td>0.867523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.771551</td>\n",
       "      <td>0.779612</td>\n",
       "      <td>0.731420</td>\n",
       "      <td>0.882920</td>\n",
       "      <td>0.897215</td>\n",
       "      <td>0.858633</td>\n",
       "      <td>0.892022</td>\n",
       "      <td>0.906002</td>\n",
       "      <td>0.865901</td>\n",
       "      <td>0.906479</td>\n",
       "      <td>0.911509</td>\n",
       "      <td>0.876279</td>\n",
       "      <td>0.892379</td>\n",
       "      <td>0.897764</td>\n",
       "      <td>0.863962</td>\n",
       "      <td>0.900589</td>\n",
       "      <td>0.911770</td>\n",
       "      <td>0.870540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.620917</td>\n",
       "      <td>0.630821</td>\n",
       "      <td>0.465904</td>\n",
       "      <td>0.864537</td>\n",
       "      <td>0.877892</td>\n",
       "      <td>0.827648</td>\n",
       "      <td>0.855435</td>\n",
       "      <td>0.872835</td>\n",
       "      <td>0.836981</td>\n",
       "      <td>0.669641</td>\n",
       "      <td>0.679846</td>\n",
       "      <td>0.547420</td>\n",
       "      <td>0.885597</td>\n",
       "      <td>0.890354</td>\n",
       "      <td>0.851442</td>\n",
       "      <td>0.851508</td>\n",
       "      <td>0.862551</td>\n",
       "      <td>0.802436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.583973</td>\n",
       "      <td>0.596535</td>\n",
       "      <td>0.427747</td>\n",
       "      <td>0.840086</td>\n",
       "      <td>0.858473</td>\n",
       "      <td>0.792843</td>\n",
       "      <td>0.833839</td>\n",
       "      <td>0.842500</td>\n",
       "      <td>0.777350</td>\n",
       "      <td>0.690880</td>\n",
       "      <td>0.708435</td>\n",
       "      <td>0.577265</td>\n",
       "      <td>0.884169</td>\n",
       "      <td>0.889795</td>\n",
       "      <td>0.851071</td>\n",
       "      <td>0.699804</td>\n",
       "      <td>0.717866</td>\n",
       "      <td>0.575719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.579154</td>\n",
       "      <td>0.587050</td>\n",
       "      <td>0.384205</td>\n",
       "      <td>0.837052</td>\n",
       "      <td>0.852810</td>\n",
       "      <td>0.781231</td>\n",
       "      <td>0.521149</td>\n",
       "      <td>0.521677</td>\n",
       "      <td>0.242842</td>\n",
       "      <td>0.672140</td>\n",
       "      <td>0.686348</td>\n",
       "      <td>0.548406</td>\n",
       "      <td>0.858112</td>\n",
       "      <td>0.869813</td>\n",
       "      <td>0.812210</td>\n",
       "      <td>0.693379</td>\n",
       "      <td>0.709665</td>\n",
       "      <td>0.563258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.522756</td>\n",
       "      <td>0.523154</td>\n",
       "      <td>0.256631</td>\n",
       "      <td>0.793503</td>\n",
       "      <td>0.811657</td>\n",
       "      <td>0.686598</td>\n",
       "      <td>0.523648</td>\n",
       "      <td>0.524726</td>\n",
       "      <td>0.246221</td>\n",
       "      <td>0.754060</td>\n",
       "      <td>0.772183</td>\n",
       "      <td>0.727128</td>\n",
       "      <td>0.849188</td>\n",
       "      <td>0.858341</td>\n",
       "      <td>0.806328</td>\n",
       "      <td>0.654114</td>\n",
       "      <td>0.670960</td>\n",
       "      <td>0.500901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.658576</td>\n",
       "      <td>0.665753</td>\n",
       "      <td>0.478660</td>\n",
       "      <td>0.515795</td>\n",
       "      <td>0.515795</td>\n",
       "      <td>0.227666</td>\n",
       "      <td>0.529002</td>\n",
       "      <td>0.530726</td>\n",
       "      <td>0.259709</td>\n",
       "      <td>0.746386</td>\n",
       "      <td>0.755956</td>\n",
       "      <td>0.668150</td>\n",
       "      <td>0.518829</td>\n",
       "      <td>0.519014</td>\n",
       "      <td>0.227786</td>\n",
       "      <td>0.624130</td>\n",
       "      <td>0.636687</td>\n",
       "      <td>0.453713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.514724</td>\n",
       "      <td>0.515661</td>\n",
       "      <td>0.238582</td>\n",
       "      <td>0.515617</td>\n",
       "      <td>0.515617</td>\n",
       "      <td>0.227208</td>\n",
       "      <td>0.515974</td>\n",
       "      <td>0.516262</td>\n",
       "      <td>0.230839</td>\n",
       "      <td>0.712297</td>\n",
       "      <td>0.716856</td>\n",
       "      <td>0.573023</td>\n",
       "      <td>0.518829</td>\n",
       "      <td>0.518829</td>\n",
       "      <td>0.227732</td>\n",
       "      <td>0.530430</td>\n",
       "      <td>0.532049</td>\n",
       "      <td>0.255857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.516688</td>\n",
       "      <td>0.518805</td>\n",
       "      <td>0.241406</td>\n",
       "      <td>0.515795</td>\n",
       "      <td>0.515979</td>\n",
       "      <td>0.227719</td>\n",
       "      <td>0.519900</td>\n",
       "      <td>0.521724</td>\n",
       "      <td>0.239614</td>\n",
       "      <td>0.697305</td>\n",
       "      <td>0.702239</td>\n",
       "      <td>0.505923</td>\n",
       "      <td>0.519543</td>\n",
       "      <td>0.519636</td>\n",
       "      <td>0.229576</td>\n",
       "      <td>0.598965</td>\n",
       "      <td>0.607499</td>\n",
       "      <td>0.410954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model1acc  model1prec  model1f1  model2acc  model2prec  model2f1  \\\n",
       "0   0.858469    0.867345  0.815639   0.892736    0.906456  0.861495   \n",
       "1   0.771551    0.779612  0.731420   0.882920    0.897215  0.858633   \n",
       "2   0.620917    0.630821  0.465904   0.864537    0.877892  0.827648   \n",
       "3   0.583973    0.596535  0.427747   0.840086    0.858473  0.792843   \n",
       "4   0.579154    0.587050  0.384205   0.837052    0.852810  0.781231   \n",
       "5   0.522756    0.523154  0.256631   0.793503    0.811657  0.686598   \n",
       "6   0.658576    0.665753  0.478660   0.515795    0.515795  0.227666   \n",
       "7   0.514724    0.515661  0.238582   0.515617    0.515617  0.227208   \n",
       "8   0.516688    0.518805  0.241406   0.515795    0.515979  0.227719   \n",
       "\n",
       "   model3acc  model3prec  model3f1  model4acc  model4prec  model4f1  \\\n",
       "0   0.907728    0.914265  0.884877   0.896306    0.903068  0.863371   \n",
       "1   0.892022    0.906002  0.865901   0.906479    0.911509  0.876279   \n",
       "2   0.855435    0.872835  0.836981   0.669641    0.679846  0.547420   \n",
       "3   0.833839    0.842500  0.777350   0.690880    0.708435  0.577265   \n",
       "4   0.521149    0.521677  0.242842   0.672140    0.686348  0.548406   \n",
       "5   0.523648    0.524726  0.246221   0.754060    0.772183  0.727128   \n",
       "6   0.529002    0.530726  0.259709   0.746386    0.755956  0.668150   \n",
       "7   0.515974    0.516262  0.230839   0.712297    0.716856  0.573023   \n",
       "8   0.519900    0.521724  0.239614   0.697305    0.702239  0.505923   \n",
       "\n",
       "   model5acc  model5prec  model5f1  model6acc  model6prec  model6f1  \n",
       "0   0.898804    0.904495  0.869921   0.896484    0.906968  0.867523  \n",
       "1   0.892379    0.897764  0.863962   0.900589    0.911770  0.870540  \n",
       "2   0.885597    0.890354  0.851442   0.851508    0.862551  0.802436  \n",
       "3   0.884169    0.889795  0.851071   0.699804    0.717866  0.575719  \n",
       "4   0.858112    0.869813  0.812210   0.693379    0.709665  0.563258  \n",
       "5   0.849188    0.858341  0.806328   0.654114    0.670960  0.500901  \n",
       "6   0.518829    0.519014  0.227786   0.624130    0.636687  0.453713  \n",
       "7   0.518829    0.518829  0.227732   0.530430    0.532049  0.255857  \n",
       "8   0.519543    0.519636  0.229576   0.598965    0.607499  0.410954  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d2ded74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save results to csv for later use\n",
    "results_df.to_csv(\"UntunedPruningResults.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305f0161",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
